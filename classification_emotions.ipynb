{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76895ba3",
   "metadata": {},
   "source": [
    "# Traitement naturel du language\n",
    "## Création d'un RNN pour la classification de phrases en six émotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f928a1",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bd428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e8eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from RNN import *\n",
    "from data_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d18893",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2761174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im grabbing a minute to post i feel greedy wrong\n",
      "['im', 'grabbing', 'a', 'minute', 'to', 'post', 'i', 'feel', 'greedy', 'wrong']\n",
      "anger\n"
     ]
    }
   ],
   "source": [
    "text, emotion = load_file(\"./dataset/train.txt\")\n",
    "\n",
    "print(text[2])\n",
    "print(tokenizer(text[2]))\n",
    "print(emotion[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680173cd",
   "metadata": {},
   "source": [
    "#### Création de l'ensemble de mots \n",
    "1. Lister tous les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5d8f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'didnt', 'feel', 'humiliated', 'i', 'can', 'go', 'from', 'feeling', 'so', 'hopeless', 'to', 'so', 'damned', 'hopeful', 'just', 'from', 'being', 'around', 'someone', 'who', 'cares', 'and', 'is', 'awake', 'im', 'grabbing', 'a', 'minute', 'to', 'post', 'i', 'feel', 'greedy', 'wrong', 'i', 'am', 'ever', 'feeling', 'nostalgic', 'about', 'the', 'fireplace', 'i', 'will', 'know', 'that', 'it', 'is', 'still']\n",
      "['i', 'feel', 'pathetic', 'that', 'i', 'am', 'still', 'waiting', 'tables', 'and', 'subbing', 'with', 'a', 'teaching', 'degree', 'i', 'feel', 'strong', 'and', 'good', 'overall', 'i', 'feel', 'like', 'this', 'was', 'such', 'a', 'rude', 'comment', 'and', 'im', 'glad', 'that', 't', 'i', 'know', 'a', 'lot', 'but', 'i', 'feel', 'so', 'stupid', 'because', 'i', 'can', 'not', 'portray', 'it']\n"
     ]
    }
   ],
   "source": [
    "lst = list(yield_tokens(text))\n",
    "print(lst[:50])\n",
    "print(lst[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25652c9d",
   "metadata": {},
   "source": [
    "2. Associer à chaque mot une valeur unique (entier positif) **pas de doublon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6ef051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15214\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(lst, specials=[\"<pad>\", \"<unk>\"])\n",
    "print(len(vocab))\n",
    "print(vocab[\"i\"])\n",
    "print(vocab[\"didnt\"])\n",
    "print(vocab[\"feel\"])\n",
    "print(vocab[\"<pad>\"])\n",
    "print(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdbdef",
   "metadata": {},
   "source": [
    "#### Faire pareil avec les émotions, qui représentent les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd675915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "classes = build_vocab_from_iterator(yield_tokens(emotion))\n",
    "print(len(classes))\n",
    "print(classes[\"anger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdb2dc",
   "metadata": {},
   "source": [
    "#### Coder une phrase \n",
    "1. Représenter une phrase comme une suite de valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab32c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "[2, 6, 7, 8, 9, 10, 11, 12, 10, 13, 14, 15, 8, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "print(text[1])\n",
    "codage_entier_phrase = [vocab[word] for word in tokenizer(text[1])]\n",
    "print(codage_entier_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6fccb2",
   "metadata": {},
   "source": [
    "2. Représenter chaque phrase comme un tensor d'entiers **seulement à partir de cette étape, on utiliser la librairie pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b4888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  6,  7,  8,  9, 10, 11, 12, 10, 13, 14, 15,  8, 16, 17, 18, 19, 20,\n",
      "        21, 22, 23])\n"
     ]
    }
   ],
   "source": [
    "tensor_entier = torch.tensor(codage_entier_phrase)\n",
    "print(tensor_entier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ff7b4",
   "metadata": {},
   "source": [
    "3. Représenter chaque phrase comme un tensor one hot **attention, il peut être difficile de charger en mémoire l'ensemble du dataset sous la forme on-hot, privilégier si besoin la génération one-hot par batch, en utilisant un Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348101ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "tensor_one_hot = torch.nn.functional.one_hot(tensor_entier, num_classes=len(vocab))\n",
    "print(tensor_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e817b",
   "metadata": {},
   "source": [
    "4. Ensuite : \n",
    "- Finaliser le codage : \n",
    "    - rogner ou compléter les phrase (*pad*) : longeur identique en entrée du réseau quelque soit la phrase \n",
    "    - optimiser : *unk* pour les mots sous représentés \n",
    "    - préparer le réseau \n",
    "    - entrainer le réseau sur la classification des émotions \n",
    "    - optimiser, analyser les résultats \n",
    "    - entrainer le réseau de façon autosupervisée : prédiction du mots suivant à partir de mots précédents par exemple \n",
    "    - analyser l'embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a439c9",
   "metadata": {},
   "source": [
    "#### Rogner ou compléter les phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455c934f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2641., 3795., 3968., 2306., 1499.,  963.,  453.,  251.,  106.,\n",
       "          18.]),\n",
       " array([ 2. ,  8.4, 14.8, 21.2, 27.6, 34. , 40.4, 46.8, 53.2, 59.6, 66. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAGsCAYAAAAWptzrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMIJJREFUeJzt3X9wVfWd//FnIOTy8yYC5tcSMC2tEPmhoA13VFZLSsTo6oo7pVJhK+rABrcQiyFbFn/0Rxhcq1gV2nW3caZQxU6xlSxgBImrBtRolh9WVilu6MJNbG1ygUL4kfP9o8v9eisoQdIk+HzMnJnc83mfc96nnzJzX557zkkKgiBAkiRJkj7junV0A5IkSZLUGRiOJEmSJAnDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEgDJHd1Ae2ltbWXPnj3069ePpKSkjm5HkiRJUgcJgoB9+/aRnZ1Nt24nvz501oajPXv2kJOT09FtSJIkSeokdu/ezaBBg046ftaGo379+gF/+h8gHA53cDeSJEmSOkosFiMnJyeeEU7mrA1Hx39KFw6HDUeSJEmSPvF2Gx/IIEmSJEkYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgR8ynC0aNEikpKSmDNnTnzdoUOHKC4uZsCAAfTt25fJkyfT0NCQsF19fT1FRUX07t2b9PR05s2bx9GjRxNqNm7cyJgxYwiFQgwdOpSKiopP06okSZIkfazTDkevvfYaP/rRjxg1alTC+rlz5/Lss8/y9NNPU11dzZ49e7jhhhvi48eOHaOoqIjDhw/zyiuv8MQTT1BRUcHChQvjNbt27aKoqIgrr7ySuro65syZw6233sq6detOt11JkiRJ+lhJQRAEbd1o//79jBkzhscee4zvfve7XHjhhTz00EM0Nzdz7rnnsmLFCm688UYA3n77bYYPH05NTQ3jxo1jzZo1XHPNNezZs4eMjAwAli1bRmlpKe+//z4pKSmUlpZSWVnJtm3b4secMmUKTU1NrF279pR6jMVipKam0tzcTDgcbuspSpIkSTpLnGo2OK0rR8XFxRQVFVFQUJCwvra2liNHjiSsHzZsGIMHD6ampgaAmpoaRo4cGQ9GAIWFhcRiMbZv3x6v+fN9FxYWxvdxIi0tLcRisYRFkiRJkk5Vcls3ePLJJ3njjTd47bXXPjIWjUZJSUkhLS0tYX1GRgbRaDRe8+FgdHz8+NjH1cRiMQ4ePEivXr0+cuzy8nLuvffetp6OJEmSJAFtvHK0e/duvvnNb7J8+XJ69uzZXj2dlrKyMpqbm+PL7t27O7olSZIkSV1Im8JRbW0tjY2NjBkzhuTkZJKTk6murubhhx8mOTmZjIwMDh8+TFNTU8J2DQ0NZGZmApCZmfmRp9cd//xJNeFw+IRXjQBCoRDhcDhhkSRJkqRT1aaf1U2YMIGtW7cmrPvGN77BsGHDKC0tJScnhx49erB+/XomT54MwI4dO6ivrycSiQAQiUT43ve+R2NjI+np6QBUVVURDofJy8uL1/zHf/xHwnGqqqri+5A+C86bX9nRLXRK7y0q6ugWJEnSWapN4ahfv36MGDEiYV2fPn0YMGBAfP2MGTMoKSmhf//+hMNh7rjjDiKRCOPGjQNg4sSJ5OXlcfPNN7N48WKi0SgLFiyguLiYUCgEwMyZM3nkkUe46667uOWWW9iwYQMrV66kstIvi5IkSZLaR5sfyPBJHnzwQbp168bkyZNpaWmhsLCQxx57LD7evXt3Vq9ezaxZs4hEIvTp04fp06dz3333xWtyc3OprKxk7ty5LFmyhEGDBvH4449TWFh4ptuVJEmSJOA033PUFfieI3V1/qzuxPxZnSRJaqt2fc+RJEmSJJ1tDEeSJEmShOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBkNzRDUjnza/s6BYkSZIkrxxJkiRJEhiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBBiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBBiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEmA4UiSJEmSgDaGo6VLlzJq1CjC4TDhcJhIJMKaNWvi41dccQVJSUkJy8yZMxP2UV9fT1FREb179yY9PZ158+Zx9OjRhJqNGzcyZswYQqEQQ4cOpaKi4vTPUJIkSZJOQXJbigcNGsSiRYv4whe+QBAEPPHEE1x33XW8+eabXHDBBQDcdttt3HffffFtevfuHf/72LFjFBUVkZmZySuvvMLevXuZNm0aPXr04Pvf/z4Au3btoqioiJkzZ7J8+XLWr1/PrbfeSlZWFoWFhWfinCVJkiTpI5KCIAg+zQ769+/P/fffz4wZM7jiiiu48MILeeihh05Yu2bNGq655hr27NlDRkYGAMuWLaO0tJT333+flJQUSktLqaysZNu2bfHtpkyZQlNTE2vXrj3lvmKxGKmpqTQ3NxMOhz/NKaqdnTe/sqNbUBfy3qKijm5BkiR1MaeaDU77nqNjx47x5JNPcuDAASKRSHz98uXLGThwICNGjKCsrIw//vGP8bGamhpGjhwZD0YAhYWFxGIxtm/fHq8pKChIOFZhYSE1NTUf209LSwuxWCxhkSRJkqRT1aaf1QFs3bqVSCTCoUOH6Nu3L6tWrSIvLw+Am266iSFDhpCdnc2WLVsoLS1lx44d/OIXvwAgGo0mBCMg/jkajX5sTSwW4+DBg/Tq1euEfZWXl3Pvvfe29XQkSZIkCTiNcHT++edTV1dHc3MzP//5z5k+fTrV1dXk5eVx++23x+tGjhxJVlYWEyZMYOfOnXz+858/o43/ubKyMkpKSuKfY7EYOTk57XpMSZIkSWePNv+sLiUlhaFDhzJ27FjKy8sZPXo0S5YsOWFtfn4+AO+++y4AmZmZNDQ0JNQc/5yZmfmxNeFw+KRXjQBCoVD8KXrHF0mSJEk6VZ/6PUetra20tLSccKyurg6ArKwsACKRCFu3bqWxsTFeU1VVRTgcjv80LxKJsH79+oT9VFVVJdzXJEmSJElnWpt+VldWVsakSZMYPHgw+/btY8WKFWzcuJF169axc+dOVqxYwdVXX82AAQPYsmULc+fOZfz48YwaNQqAiRMnkpeXx80338zixYuJRqMsWLCA4uJiQqEQADNnzuSRRx7hrrvu4pZbbmHDhg2sXLmSykqfaCZJkiSp/bQpHDU2NjJt2jT27t1Lamoqo0aNYt26dXzlK19h9+7dPP/88zz00EMcOHCAnJwcJk+ezIIFC+Lbd+/endWrVzNr1iwikQh9+vRh+vTpCe9Fys3NpbKykrlz57JkyRIGDRrE448/7juOJEmSJLWrT/2eo87K9xx1Hb7nSG3he44kSVJbtft7jiRJkiTpbGI4kiRJkiQMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEtDEcLV26lFGjRhEOhwmHw0QiEdasWRMfP3ToEMXFxQwYMIC+ffsyefJkGhoaEvZRX19PUVERvXv3Jj09nXnz5nH06NGEmo0bNzJmzBhCoRBDhw6loqLi9M9QkiRJkk5Bm8LRoEGDWLRoEbW1tbz++ut8+ctf5rrrrmP79u0AzJ07l2effZann36a6upq9uzZww033BDf/tixYxQVFXH48GFeeeUVnnjiCSoqKli4cGG8ZteuXRQVFXHllVdSV1fHnDlzuPXWW1m3bt0ZOmVJkiRJ+qikIAiCT7OD/v37c//993PjjTdy7rnnsmLFCm688UYA3n77bYYPH05NTQ3jxo1jzZo1XHPNNezZs4eMjAwAli1bRmlpKe+//z4pKSmUlpZSWVnJtm3b4seYMmUKTU1NrF279pT7isVipKam0tzcTDgc/jSnqHZ23vzKjm5BXch7i4o6ugVJktTFnGo2OO17jo4dO8aTTz7JgQMHiEQi1NbWcuTIEQoKCuI1w4YNY/DgwdTU1ABQU1PDyJEj48EIoLCwkFgsFr/6VFNTk7CP4zXH93EyLS0txGKxhEWSJEmSTlWbw9HWrVvp27cvoVCImTNnsmrVKvLy8ohGo6SkpJCWlpZQn5GRQTQaBSAajSYEo+Pjx8c+riYWi3Hw4MGT9lVeXk5qamp8ycnJaeupSZIkSfoMa3M4Ov/886mrq2Pz5s3MmjWL6dOn89Zbb7VHb21SVlZGc3NzfNm9e3dHtyRJkiSpC0lu6wYpKSkMHToUgLFjx/Laa6+xZMkSvvrVr3L48GGampoSrh41NDSQmZkJQGZmJq+++mrC/o4/ze7DNX/+hLuGhgbC4TC9evU6aV+hUIhQKNTW05EkSZIk4Ay856i1tZWWlhbGjh1Ljx49WL9+fXxsx44d1NfXE4lEAIhEImzdupXGxsZ4TVVVFeFwmLy8vHjNh/dxvOb4PiRJkiSpPbTpylFZWRmTJk1i8ODB7Nu3jxUrVrBx40bWrVtHamoqM2bMoKSkhP79+xMOh7njjjuIRCKMGzcOgIkTJ5KXl8fNN9/M4sWLiUajLFiwgOLi4vhVn5kzZ/LII49w1113ccstt7BhwwZWrlxJZaVPNJMkSZLUftoUjhobG5k2bRp79+4lNTWVUaNGsW7dOr7yla8A8OCDD9KtWzcmT55MS0sLhYWFPPbYY/Htu3fvzurVq5k1axaRSIQ+ffowffp07rvvvnhNbm4ulZWVzJ07lyVLljBo0CAef/xxCgsLz9ApS5IkSdJHfer3HHVWvueo6/A9R2oL33MkSZLaqt3fcyRJkiRJZxPDkSRJkiRhOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRIAyR3dwGfFefMrO7oFSZIkSR/DK0eSJEmShOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiSgjeGovLycSy65hH79+pGens7111/Pjh07EmquuOIKkpKSEpaZM2cm1NTX11NUVETv3r1JT09n3rx5HD16NKFm48aNjBkzhlAoxNChQ6moqDi9M5QkSZKkU9CmcFRdXU1xcTGbNm2iqqqKI0eOMHHiRA4cOJBQd9ttt7F37974snjx4vjYsWPHKCoq4vDhw7zyyis88cQTVFRUsHDhwnjNrl27KCoq4sorr6Suro45c+Zw6623sm7duk95upIkSZJ0YsltKV67dm3C54qKCtLT06mtrWX8+PHx9b179yYzM/OE+3juued46623eP7558nIyODCCy/kO9/5DqWlpdxzzz2kpKSwbNkycnNzeeCBBwAYPnw4L730Eg8++CCFhYVtPUdJkiRJ+kSf6p6j5uZmAPr375+wfvny5QwcOJARI0ZQVlbGH//4x/hYTU0NI0eOJCMjI76usLCQWCzG9u3b4zUFBQUJ+ywsLKSmpuakvbS0tBCLxRIWSZIkSTpVbbpy9GGtra3MmTOHSy+9lBEjRsTX33TTTQwZMoTs7Gy2bNlCaWkpO3bs4Be/+AUA0Wg0IRgB8c/RaPRja2KxGAcPHqRXr14f6ae8vJx77733dE9HkiRJ0mfcaYej4uJitm3bxksvvZSw/vbbb4//PXLkSLKyspgwYQI7d+7k85///Ol3+gnKysooKSmJf47FYuTk5LTb8SRJkiSdXU7rZ3WzZ89m9erVvPDCCwwaNOhja/Pz8wF49913AcjMzKShoSGh5vjn4/cpnawmHA6f8KoRQCgUIhwOJyySJEmSdKraFI6CIGD27NmsWrWKDRs2kJub+4nb1NXVAZCVlQVAJBJh69atNDY2xmuqqqoIh8Pk5eXFa9avX5+wn6qqKiKRSFvalSRJkqRT1qZwVFxczE9/+lNWrFhBv379iEajRKNRDh48CMDOnTv5zne+Q21tLe+99x6/+tWvmDZtGuPHj2fUqFEATJw4kby8PG6++Wb+67/+i3Xr1rFgwQKKi4sJhUIAzJw5k9/85jfcddddvP322zz22GOsXLmSuXPnnuHTlyRJkqQ/aVM4Wrp0Kc3NzVxxxRVkZWXFl6eeegqAlJQUnn/+eSZOnMiwYcO48847mTx5Ms8++2x8H927d2f16tV0796dSCTC17/+daZNm8Z9990Xr8nNzaWyspKqqipGjx7NAw88wOOPP+5jvCVJkiS1m6QgCIKObqI9xGIxUlNTaW5u7hT3H503v7KjW5DOCu8tKuroFiRJUhdzqtngU73nSJIkSZLOFoYjSZIkScJwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBBiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEkAJHd0A5LUFufNr+zoFjqt9xYVdXQLkiR1aV45kiRJkiQMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAW0MR+Xl5VxyySX069eP9PR0rr/+enbs2JFQc+jQIYqLixkwYAB9+/Zl8uTJNDQ0JNTU19dTVFRE7969SU9PZ968eRw9ejShZuPGjYwZM4ZQKMTQoUOpqKg4vTOUJEmSpFPQpnBUXV1NcXExmzZtoqqqiiNHjjBx4kQOHDgQr5k7dy7PPvssTz/9NNXV1ezZs4cbbrghPn7s2DGKioo4fPgwr7zyCk888QQVFRUsXLgwXrNr1y6Kioq48sorqaurY86cOdx6662sW7fuDJyyJEmSJH1UUhAEwelu/P7775Oenk51dTXjx4+nubmZc889lxUrVnDjjTcC8PbbbzN8+HBqamoYN24ca9as4ZprrmHPnj1kZGQAsGzZMkpLS3n//fdJSUmhtLSUyspKtm3bFj/WlClTaGpqYu3atSfspaWlhZaWlvjnWCxGTk4Ozc3NhMPh0z3FM+a8+ZUd3YKks9x7i4o6ugVJkjqlWCxGamrqJ2aDT3XPUXNzMwD9+/cHoLa2liNHjlBQUBCvGTZsGIMHD6ampgaAmpoaRo4cGQ9GAIWFhcRiMbZv3x6v+fA+jtcc38eJlJeXk5qaGl9ycnI+zalJkiRJ+ow57XDU2trKnDlzuPTSSxkxYgQA0WiUlJQU0tLSEmozMjKIRqPxmg8Ho+Pjx8c+riYWi3Hw4MET9lNWVkZzc3N82b179+memiRJkqTPoOTT3bC4uJht27bx0ksvncl+TlsoFCIUCnV0G5IkSZK6qNO6cjR79mxWr17NCy+8wKBBg+LrMzMzOXz4ME1NTQn1DQ0NZGZmxmv+/Ol1xz9/Uk04HKZXr16n07IkSZIkfaw2haMgCJg9ezarVq1iw4YN5ObmJoyPHTuWHj16sH79+vi6HTt2UF9fTyQSASASibB161YaGxvjNVVVVYTDYfLy8uI1H97H8Zrj+5AkSZKkM61NP6srLi5mxYoV/PKXv6Rfv37xe4RSU1Pp1asXqampzJgxg5KSEvr37084HOaOO+4gEokwbtw4ACZOnEheXh4333wzixcvJhqNsmDBAoqLi+M/i5s5cyaPPPIId911F7fccgsbNmxg5cqVVFb6xDdJkiRJ7aNNV46WLl1Kc3MzV1xxBVlZWfHlqaeeitc8+OCDXHPNNUyePJnx48eTmZnJL37xi/h49+7dWb16Nd27dycSifD1r3+dadOmcd9998VrcnNzqayspKqqitGjR/PAAw/w+OOPU1hYeAZOWZIkSZI+6lO956gzO9Vnmf+l+J4jSe3N9xxJknRif5H3HEmSJEnS2cJwJEmSJEkYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJOI1w9OKLL3LttdeSnZ1NUlISzzzzTML43//935OUlJSwXHXVVQk1H3zwAVOnTiUcDpOWlsaMGTPYv39/Qs2WLVu4/PLL6dmzJzk5OSxevLjtZydJkiRJp6jN4ejAgQOMHj2aRx999KQ1V111FXv37o0vP/vZzxLGp06dyvbt26mqqmL16tW8+OKL3H777fHxWCzGxIkTGTJkCLW1tdx///3cc889/PjHP25ru5IkSZJ0SpLbusGkSZOYNGnSx9aEQiEyMzNPOPbrX/+atWvX8tprr3HxxRcD8MMf/pCrr76af/mXfyE7O5vly5dz+PBh/v3f/52UlBQuuOAC6urq+MEPfpAQoiRJkiTpTGmXe442btxIeno6559/PrNmzeL3v/99fKympoa0tLR4MAIoKCigW7dubN68OV4zfvx4UlJS4jWFhYXs2LGDP/zhDyc8ZktLC7FYLGGRJEmSpFPV5itHn+Sqq67ihhtuIDc3l507d/JP//RPTJo0iZqaGrp37040GiU9PT2xieRk+vfvTzQaBSAajZKbm5tQk5GRER8755xzPnLc8vJy7r333jN9OpLUZZw3v7KjW+iU3ltU1NEtSJK6iDMejqZMmRL/e+TIkYwaNYrPf/7zbNy4kQkTJpzpw8WVlZVRUlIS/xyLxcjJyWm340mSJEk6u7T7o7w/97nPMXDgQN59910AMjMzaWxsTKg5evQoH3zwQfw+pczMTBoaGhJqjn8+2b1MoVCIcDicsEiSJEnSqWr3cPTb3/6W3//+92RlZQEQiURoamqitrY2XrNhwwZaW1vJz8+P17z44oscOXIkXlNVVcX5559/wp/USZIkSdKn1eZwtH//furq6qirqwNg165d1NXVUV9fz/79+5k3bx6bNm3ivffeY/369Vx33XUMHTqUwsJCAIYPH85VV13FbbfdxquvvsrLL7/M7NmzmTJlCtnZ2QDcdNNNpKSkMGPGDLZv385TTz3FkiVLEn42J0mSJElnUpvD0euvv85FF13ERRddBEBJSQkXXXQRCxcupHv37mzZsoW/+Zu/4Ytf/CIzZsxg7Nix/Od//iehUCi+j+XLlzNs2DAmTJjA1VdfzWWXXZbwDqPU1FSee+45du3axdixY7nzzjtZuHChj/GWJEmS1G6SgiAIOrqJ9hCLxUhNTaW5ublT3H/kU6QkqWP4tDpJ0qlmg3a/50iSJEmSugLDkSRJkiRhOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIk4DTC0Ysvvsi1115LdnY2SUlJPPPMMwnjQRCwcOFCsrKy6NWrFwUFBbzzzjsJNR988AFTp04lHA6TlpbGjBkz2L9/f0LNli1buPzyy+nZsyc5OTksXry47WcnSZIkSaeozeHowIEDjB49mkcfffSE44sXL+bhhx9m2bJlbN68mT59+lBYWMihQ4fiNVOnTmX79u1UVVWxevVqXnzxRW6//fb4eCwWY+LEiQwZMoTa2lruv/9+7rnnHn784x+fxilKkiRJ0idLCoIgOO2Nk5JYtWoV119/PfCnq0bZ2dnceeedfOtb3wKgubmZjIwMKioqmDJlCr/+9a/Jy8vjtdde4+KLLwZg7dq1XH311fz2t78lOzubpUuX8u1vf5toNEpKSgoA8+fP55lnnuHtt98+pd5isRipqak0NzcTDodP9xTPmPPmV3Z0C5L0mfTeoqKObkGS1MFONRuc0XuOdu3aRTQapaCgIL4uNTWV/Px8ampqAKipqSEtLS0ejAAKCgro1q0bmzdvjteMHz8+HowACgsL2bFjB3/4wx9OeOyWlhZisVjCIkmSJEmn6oyGo2g0CkBGRkbC+oyMjPhYNBolPT09YTw5OZn+/fsn1JxoHx8+xp8rLy8nNTU1vuTk5Hz6E5IkSZL0mXHWPK2urKyM5ubm+LJ79+6ObkmSJElSF3JGw1FmZiYADQ0NCesbGhriY5mZmTQ2NiaMHz16lA8++CCh5kT7+PAx/lwoFCIcDicskiRJknSqks/kznJzc8nMzGT9+vVceOGFwJ9uftq8eTOzZs0CIBKJ0NTURG1tLWPHjgVgw4YNtLa2kp+fH6/59re/zZEjR+jRowcAVVVVnH/++ZxzzjlnsmVJ0lnOB+KcnA+rkKREbb5ytH//furq6qirqwP+9BCGuro66uvrSUpKYs6cOXz3u9/lV7/6FVu3bmXatGlkZ2fHn2g3fPhwrrrqKm677TZeffVVXn75ZWbPns2UKVPIzs4G4KabbiIlJYUZM2awfft2nnrqKZYsWUJJSckZO3FJkiRJ+rA2Xzl6/fXXufLKK+OfjweW6dOnU1FRwV133cWBAwe4/fbbaWpq4rLLLmPt2rX07Nkzvs3y5cuZPXs2EyZMoFu3bkyePJmHH344Pp6amspzzz1HcXExY8eOZeDAgSxcuDDhXUiSJEmSdCZ9qvccdWa+50iSpI/nz+okfVZ0yHuOJEmSJKmrMhxJkiRJEoYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQIMR5IkSZIEGI4kSZIkCTAcSZIkSRJgOJIkSZIkwHAkSZIkSYDhSJIkSZIAw5EkSZIkAYYjSZIkSQLaIRzdc889JCUlJSzDhg2Ljx86dIji4mIGDBhA3759mTx5Mg0NDQn7qK+vp6ioiN69e5Oens68efM4evTomW5VkiRJkuKS22OnF1xwAc8///z/P0jy/z/M3Llzqays5OmnnyY1NZXZs2dzww038PLLLwNw7NgxioqKyMzM5JVXXmHv3r1MmzaNHj168P3vf7892pUkSZKk9glHycnJZGZmfmR9c3Mz//Zv/8aKFSv48pe/DMBPfvIThg8fzqZNmxg3bhzPPfccb731Fs8//zwZGRlceOGFfOc736G0tJR77rmHlJSU9mhZkiRJ0mdcu9xz9M4775Cdnc3nPvc5pk6dSn19PQC1tbUcOXKEgoKCeO2wYcMYPHgwNTU1ANTU1DBy5EgyMjLiNYWFhcRiMbZv337SY7a0tBCLxRIWSZIkSTpVZzwc5efnU1FRwdq1a1m6dCm7du3i8ssvZ9++fUSjUVJSUkhLS0vYJiMjg2g0CkA0Gk0IRsfHj4+dTHl5OampqfElJyfnzJ6YJEmSpLPaGf9Z3aRJk+J/jxo1ivz8fIYMGcLKlSvp1avXmT5cXFlZGSUlJfHPsVjMgCRJkiTplLX7o7zT0tL44he/yLvvvktmZiaHDx+mqakpoaahoSF+j1JmZuZHnl53/POJ7mM6LhQKEQ6HExZJkiRJOlXtHo7279/Pzp07ycrKYuzYsfTo0YP169fHx3fs2EF9fT2RSASASCTC1q1baWxsjNdUVVURDofJy8tr73YlSZIkfUad8Z/Vfetb3+Laa69lyJAh7Nmzh7vvvpvu3bvzta99jdTUVGbMmEFJSQn9+/cnHA5zxx13EIlEGDduHAATJ04kLy+Pm2++mcWLFxONRlmwYAHFxcWEQqEz3a4kSZ9Z582v7OgWOqX3FhV1dAuSOsgZD0e//e1v+drXvsbvf/97zj33XC677DI2bdrEueeeC8CDDz5It27dmDx5Mi0tLRQWFvLYY4/Ft+/evTurV69m1qxZRCIR+vTpw/Tp07nvvvvOdKuSJEmSFJcUBEHQ0U20h1gsRmpqKs3NzZ3i/iP/65wkSV2DV46ks8+pZoN2v+dIkiRJkroCw5EkSZIkYTiSJEmSJMBwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBBiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBEByRzcgSZLUmZw3v7KjW+i03ltU1NEtSO3KK0eSJEmShOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJMBxJkiRJEmA4kiRJkiTAcCRJkiRJgOFIkiRJkgDDkSRJkiQBhiNJkiRJAiC5oxuQJElS13De/MqObqFTem9RUUe3oDPEK0eSJEmShOFIkiRJkgDDkSRJkiQBhiNJkiRJAgxHkiRJkgQYjiRJkiQJ6OTh6NFHH+W8886jZ8+e5Ofn8+qrr3Z0S5IkSZLOUp02HD311FOUlJRw991388YbbzB69GgKCwtpbGzs6NYkSZIknYWSgiAIOrqJE8nPz+eSSy7hkUceAaC1tZWcnBzuuOMO5s+f/5H6lpYWWlpa4p+bm5sZPHgwu3fvJhwO/8X6PpkRd6/r6BYkSZKkv6ht9xZ2dAsAxGIxcnJyaGpqIjU19aR1yX/Bnk7Z4cOHqa2tpaysLL6uW7duFBQUUFNTc8JtysvLuffeez+yPicnp936lCRJknRyqQ91dAeJ9u3b1/XC0e9+9zuOHTtGRkZGwvqMjAzefvvtE25TVlZGSUlJ/HNraysffPABAwYMICkp6ZSOezxRdparTTo1zlvX5Lx1Tc5b1+S8dU3OW9fl3HU+QRCwb98+srOzP7auU4aj0xEKhQiFQgnr0tLSTmtf4XDY/yN3Qc5b1+S8dU3OW9fkvHVNzlvX5dx1Lh93xei4TvlAhoEDB9K9e3caGhoS1jc0NJCZmdlBXUmSJEk6m3XKcJSSksLYsWNZv359fF1rayvr168nEol0YGeSJEmSzlad9md1JSUlTJ8+nYsvvpgvfelLPPTQQxw4cIBvfOMb7XbMUCjE3Xff/ZGf56lzc966Jueta3LeuibnrWty3rou567r6rSP8gZ45JFHuP/++4lGo1x44YU8/PDD5Ofnd3RbkiRJks5CnTocSZIkSdJfSqe850iSJEmS/tIMR5IkSZKE4UiSJEmSAMORJEmSJAGGowSPPvoo5513Hj179iQ/P59XX321o1vSh7z44otce+21ZGdnk5SUxDPPPJMwHgQBCxcuJCsri169elFQUMA777zTMc0KgPLyci655BL69etHeno6119/PTt27EioOXToEMXFxQwYMIC+ffsyefLkj7wAWn95S5cuZdSoUfG3u0ciEdasWRMfd946v0WLFpGUlMScOXPi65y3zumee+4hKSkpYRk2bFh83HnrvP73f/+Xr3/96wwYMIBevXoxcuRIXn/99fi43026HsPR/3nqqacoKSnh7rvv5o033mD06NEUFhbS2NjY0a3p/xw4cIDRo0fz6KOPnnB88eLFPPzwwyxbtozNmzfTp08fCgsLOXTo0F+4Ux1XXV1NcXExmzZtoqqqiiNHjjBx4kQOHDgQr5k7dy7PPvssTz/9NNXV1ezZs4cbbrihA7sWwKBBg1i0aBG1tbW8/vrrfPnLX+a6665j+/btgPPW2b322mv86Ec/YtSoUQnrnbfO64ILLmDv3r3x5aWXXoqPOW+d0x/+8AcuvfRSevTowZo1a3jrrbd44IEHOOecc+I1fjfpggIFQRAEX/rSl4Li4uL452PHjgXZ2dlBeXl5B3alkwGCVatWxT+3trYGmZmZwf333x9f19TUFIRCoeBnP/tZB3SoE2lsbAyAoLq6OgiCP81Rjx49gqeffjpe8+tf/zoAgpqamo5qUydxzjnnBI8//rjz1snt27cv+MIXvhBUVVUFf/3Xfx1885vfDILAf2+d2d133x2MHj36hGPOW+dVWloaXHbZZScd97tJ1+SVI+Dw4cPU1tZSUFAQX9etWzcKCgqoqanpwM50qnbt2kU0Gk2Yw9TUVPLz853DTqS5uRmA/v37A1BbW8uRI0cS5m3YsGEMHjzYeetEjh07xpNPPsmBAweIRCLOWydXXFxMUVFRwvyA/946u3feeYfs7Gw+97nPMXXqVOrr6wHnrTP71a9+xcUXX8zf/d3fkZ6ezkUXXcS//uu/xsf9btI1GY6A3/3udxw7doyMjIyE9RkZGUSj0Q7qSm1xfJ6cw86rtbWVOXPmcOmllzJixAjgT/OWkpJCWlpaQq3z1jls3bqVvn37EgqFmDlzJqtWrSIvL89568SefPJJ3njjDcrLyz8y5rx1Xvn5+VRUVLB27VqWLl3Krl27uPzyy9m3b5/z1on95je/YenSpXzhC19g3bp1zJo1i3/8x3/kiSeeAPxu0lUld3QDkj4biouL2bZtW8Lv6NW5nX/++dTV1dHc3MzPf/5zpk+fTnV1dUe3pZPYvXs33/zmN6mqqqJnz54d3Y7aYNKkSfG/R40aRX5+PkOGDGHlypX06tWrAzvTx2ltbeXiiy/m+9//PgAXXXQR27ZtY9myZUyfPr2Du9Pp8soRMHDgQLp37/6RJ780NDSQmZnZQV2pLY7Pk3PYOc2ePZvVq1fzwgsvMGjQoPj6zMxMDh8+TFNTU0K989Y5pKSkMHToUMaOHUt5eTmjR49myZIlzlsnVVtbS2NjI2PGjCE5OZnk5GSqq6t5+OGHSU5OJiMjw3nrItLS0vjiF7/Iu+++67+3TiwrK4u8vLyEdcOHD4//JNLvJl2T4Yg/fQEYO3Ys69evj69rbW1l/fr1RCKRDuxMpyo3N5fMzMyEOYzFYmzevNk57EBBEDB79mxWrVrFhg0byM3NTRgfO3YsPXr0SJi3HTt2UF9f77x1Qq2trbS0tDhvndSECRPYunUrdXV18eXiiy9m6tSp8b+dt65h//797Ny5k6ysLP+9dWKXXnrpR15P8d///d8MGTIE8LtJl9XRT4ToLJ588skgFAoFFRUVwVtvvRXcfvvtQVpaWhCNRju6Nf2fffv2BW+++Wbw5ptvBkDwgx/8IHjzzTeD//mf/wmCIAgWLVoUpKWlBb/85S+DLVu2BNddd12Qm5sbHDx4sIM7/+yaNWtWkJqaGmzcuDHYu3dvfPnjH/8Yr5k5c2YwePDgYMOGDcHrr78eRCKRIBKJdGDXCoIgmD9/flBdXR3s2rUr2LJlSzB//vwgKSkpeO6554IgcN66ig8/rS4InLfO6s477ww2btwY7Nq1K3j55ZeDgoKCYODAgUFjY2MQBM5bZ/Xqq68GycnJwfe+973gnXfeCZYvXx707t07+OlPfxqv8btJ12M4+pAf/vCHweDBg4OUlJTgS1/6UrBp06aObkkf8sILLwTAR5bp06cHQfCnR2b+8z//c5CRkRGEQqFgwoQJwY4dOzq26c+4E80XEPzkJz+J1xw8eDD4h3/4h+Ccc84JevfuHfzt3/5tsHfv3o5rWkEQBMEtt9wSDBkyJEhJSQnOPffcYMKECfFgFATOW1fx5+HIeeucvvrVrwZZWVlBSkpK8Fd/9VfBV7/61eDdd9+Njztvndezzz4bjBgxIgiFQsGwYcOCH//4xwnjfjfpepKCIAg65pqVJEmSJHUe3nMkSZIkSRiOJEmSJAkwHEmSJEkSYDiSJEmSJMBwJEmSJEmA4UiSJEmSAMORJEmSJAGGI0mSJEkCDEeSJEmSBBiOJEmSJAkwHEmSJEkSAP8P5nrGQuAdAM8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"sentences_lenght = {}\n",
    "for sentence in text:\n",
    "    size = len(sentence)\n",
    "    sentences_lenght[size] = sentences_lenght.get(size, 0) + 1\"\"\"\n",
    "\n",
    "sentences_lenght = [len(sentence.split(\" \")) for sentence in text]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(sentences_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07718a03",
   "metadata": {},
   "source": [
    "Au vue de la distribution de la taille des phrases, on peut considérer que ne garder que les phrases de taille inférieure à 20 sera déjà un bon départ pour le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ce8a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'didnt', 'feel', 'humiliated', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'im', 'grabbing', 'a', 'minute', 'to', 'post', 'i', 'feel', 'greedy', 'wrong', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'i', 'am', 'ever', 'feeling', 'nostalgic', 'about', 'the', 'fireplace', 'i', 'will']\n",
      "197880\n"
     ]
    }
   ],
   "source": [
    "size_threshold = 20\n",
    "lst = list(yield_tokens(text, threshold=size_threshold))\n",
    "print(lst[:50])\n",
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e241ff",
   "metadata": {},
   "source": [
    "#### Unknown pour les mots sous-représentés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1416dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'didnt',\n",
       " 'feel',\n",
       " 'humiliated',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'im',\n",
       " '<unk>',\n",
       " 'a',\n",
       " 'minute',\n",
       " 'to',\n",
       " 'post',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'greedy',\n",
       " 'wrong',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'ever',\n",
       " 'feeling',\n",
       " 'nostalgic',\n",
       " 'about',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'i',\n",
       " 'will',\n",
       " 'know',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'still',\n",
       " 'on',\n",
       " 'the',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'feeling',\n",
       " 'grouchy',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'feeling',\n",
       " 'a',\n",
       " 'little',\n",
       " 'burdened',\n",
       " 'lately',\n",
       " 'wasnt',\n",
       " 'sure',\n",
       " 'why',\n",
       " 'that',\n",
       " 'was',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'as',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'life',\n",
       " 'as',\n",
       " 'a',\n",
       " 'teenager',\n",
       " 'or',\n",
       " 'as',\n",
       " 'jaded',\n",
       " 'as',\n",
       " 'a',\n",
       " 'year',\n",
       " 'old',\n",
       " 'man',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'with',\n",
       " '<unk>',\n",
       " 'for',\n",
       " 'years',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'that',\n",
       " '<unk>',\n",
       " 'has',\n",
       " '<unk>',\n",
       " 'well',\n",
       " 'and',\n",
       " 'made',\n",
       " 'a',\n",
       " 'huge',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'romantic',\n",
       " 'too',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'suffering',\n",
       " 'i',\n",
       " 'm',\n",
       " 'seeing',\n",
       " 'mean',\n",
       " 'something',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'think',\n",
       " 'it',\n",
       " 's',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'time',\n",
       " 'of',\n",
       " 'year',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'dissatisfied',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'low',\n",
       " 'energy',\n",
       " 'i',\n",
       " 'm',\n",
       " 'just',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'feel',\n",
       " 'reassured',\n",
       " 'anxiety',\n",
       " 'is',\n",
       " 'on',\n",
       " 'each',\n",
       " 'side',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'didnt',\n",
       " 'really',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'embarrassed',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'pretty',\n",
       " 'pathetic',\n",
       " 'most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'time',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'now',\n",
       " 'feel',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'skeptical',\n",
       " 'of',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'of',\n",
       " 'every',\n",
       " '<unk>',\n",
       " 'of',\n",
       " 'work',\n",
       " 'i',\n",
       " 'put',\n",
       " 'in',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'irritated',\n",
       " 'and',\n",
       " 'rejected',\n",
       " 'without',\n",
       " 'anyone',\n",
       " 'doing',\n",
       " 'anything',\n",
       " 'or',\n",
       " 'saying',\n",
       " 'anything',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'the',\n",
       " 'feeling',\n",
       " 'she',\n",
       " 'was',\n",
       " 'amused',\n",
       " 'and',\n",
       " 'delighted',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'already',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'i',\n",
       " 'fucked',\n",
       " 'up',\n",
       " 'though',\n",
       " 'because',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'usually',\n",
       " 'eat',\n",
       " 'at',\n",
       " 'all',\n",
       " 'in',\n",
       " 'the',\n",
       " 'morning',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'so',\n",
       " 'inhibited',\n",
       " 'in',\n",
       " 'someone',\n",
       " '<unk>',\n",
       " 'kitchen',\n",
       " 'like',\n",
       " 'im',\n",
       " 'painting',\n",
       " 'on',\n",
       " 'someone',\n",
       " '<unk>',\n",
       " 'picture',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'become',\n",
       " 'overwhelmed',\n",
       " 'and',\n",
       " 'feel',\n",
       " 'defeated',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'more',\n",
       " 'superior',\n",
       " 'dead',\n",
       " 'chicken',\n",
       " 'or',\n",
       " '<unk>',\n",
       " 'child',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'get',\n",
       " '<unk>',\n",
       " 'over',\n",
       " 'feeling',\n",
       " 'elegant',\n",
       " 'in',\n",
       " 'a',\n",
       " 'perfectly',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'feeling',\n",
       " '<unk>',\n",
       " 'distressed',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'days',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'what',\n",
       " 'would',\n",
       " 'make',\n",
       " 'me',\n",
       " 'feel',\n",
       " 'content',\n",
       " 'if',\n",
       " 'anything',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'feeling',\n",
       " 'the',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'creative',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'strongly',\n",
       " 'passionate',\n",
       " 'about',\n",
       " 'when',\n",
       " 'some',\n",
       " '<unk>',\n",
       " 'off',\n",
       " '<unk>',\n",
       " 'to',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'make',\n",
       " 'fun',\n",
       " 'of',\n",
       " 'us',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'was',\n",
       " 'feeling',\n",
       " 'listless',\n",
       " 'from',\n",
       " 'the',\n",
       " 'need',\n",
       " 'of',\n",
       " 'new',\n",
       " 'things',\n",
       " 'something',\n",
       " 'different',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'on',\n",
       " 'a',\n",
       " '<unk>',\n",
       " 'trip',\n",
       " 'to',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'stopped',\n",
       " 'feeling',\n",
       " 'cold',\n",
       " 'and',\n",
       " 'began',\n",
       " 'feeling',\n",
       " 'hot',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'its',\n",
       " 'just',\n",
       " 'perfect',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'found',\n",
       " 'myself',\n",
       " 'feeling',\n",
       " 'a',\n",
       " 'little',\n",
       " 'discouraged',\n",
       " 'that',\n",
       " 'morning',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'selfish',\n",
       " 'and',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'was',\n",
       " '<unk>',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'as',\n",
       " 'i',\n",
       " 'wrote',\n",
       " 'feeling',\n",
       " 'unsure',\n",
       " 'that',\n",
       " 'i',\n",
       " 'might',\n",
       " 'go',\n",
       " 'somewhere',\n",
       " 'with',\n",
       " 'the',\n",
       " 'story',\n",
       " '<unk>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'you',\n",
       " 'know',\n",
       " 'basically',\n",
       " 'like',\n",
       " 'a',\n",
       " 'fake',\n",
       " 'in',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'of',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'feeling',\n",
       " 'pleasantly',\n",
       " 'surprised',\n",
       " 'at',\n",
       " 'his',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'also',\n",
       " 'his',\n",
       " 'ease',\n",
       " 'in',\n",
       " 'new',\n",
       " 'situations',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'my',\n",
       " 'mom',\n",
       " 's',\n",
       " 'graceful',\n",
       " 'warm',\n",
       " 'loving',\n",
       " 'smile',\n",
       " 'as',\n",
       " 'i',\n",
       " '<unk>',\n",
       " 'the',\n",
       " 'time',\n",
       " 'to',\n",
       " '<unk>',\n",
       " 'myself',\n",
       " 'and',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'began',\n",
       " 'having',\n",
       " 'them',\n",
       " 'several',\n",
       " 'times',\n",
       " 'a',\n",
       " 'week',\n",
       " 'feeling',\n",
       " 'tortured',\n",
       " 'by',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'moving',\n",
       " 'people',\n",
       " 'and',\n",
       " '<unk>',\n",
       " 'sounds',\n",
       " 'and',\n",
       " '<unk>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'now',\n",
       " 'nearly',\n",
       " 'finished',\n",
       " 'the',\n",
       " 'week',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'amazing',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'feel',\n",
       " 'when',\n",
       " 'an',\n",
       " 'enraged',\n",
       " 'child',\n",
       " 'becomes',\n",
       " 'violent',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'on',\n",
       " 'a',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 'of',\n",
       " 'emotions',\n",
       " 'over',\n",
       " 'these',\n",
       " 'supposed',\n",
       " 'feelings',\n",
       " 'that',\n",
       " 'something',\n",
       " 'unpleasant',\n",
       " 'was',\n",
       " 'coming',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'was',\n",
       " 'feeling',\n",
       " 'brave',\n",
       " 'when',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'it',\n",
       " 'and',\n",
       " 'clearly',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'doing',\n",
       " 'my',\n",
       " 'makeup',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'feeling',\n",
       " 'miserable',\n",
       " 'but',\n",
       " 'c',\n",
       " 'i',\n",
       " 'am',\n",
       " 'also',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'mum',\n",
       " 'on',\n",
       " 'earth',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'figure',\n",
       " 'my',\n",
       " 'family',\n",
       " 'loves',\n",
       " 'us',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'what',\n",
       " 'but',\n",
       " 'around',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'embarrassed',\n",
       " 'when',\n",
       " '<unk>',\n",
       " 'goes',\n",
       " '<unk>',\n",
       " 'i',\n",
       " 'can',\n",
       " 'feel',\n",
       " 'my',\n",
       " '<unk>',\n",
       " 'aching',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'me',\n",
       " 'as',\n",
       " 'i',\n",
       " 'like',\n",
       " 'to',\n",
       " 'put',\n",
       " 'it',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'didn',\n",
       " 't',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'doing',\n",
       " 'much',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'i',\n",
       " 'mostly',\n",
       " 'just',\n",
       " 'took',\n",
       " 'too',\n",
       " 'many',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'unimportant',\n",
       " 'stuff',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'think',\n",
       " 'feelings',\n",
       " 'are',\n",
       " 'one',\n",
       " 'of',\n",
       " '<unk>',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'things',\n",
       " 'we',\n",
       " 'have',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'completely',\n",
       " 'honored',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " '<unk>',\n",
       " 'to',\n",
       " 'this',\n",
       " 'young',\n",
       " 'talented',\n",
       " 'fully',\n",
       " 'alive',\n",
       " 'beautiful',\n",
       " 'girl',\n",
       " 'woman',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'angered',\n",
       " 'and',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'a',\n",
       " 'miserable',\n",
       " 'piece',\n",
       " 'of',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'feeling',\n",
       " 'so',\n",
       " '<unk>',\n",
       " 'furious',\n",
       " 'with',\n",
       " 'the',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appearance_threshold = 3\n",
    "lst_with_unk = yield_tokens_with_unknown(lst, threshold=appearance_threshold)\n",
    "lst_with_unk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a6674",
   "metadata": {},
   "source": [
    "#### Préparer le réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1727b0",
   "metadata": {},
   "source": [
    "1. Batch des données de train (DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075291d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for filtering the vocabulary\n",
    "size_threshold = 20\n",
    "appearance_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abd6769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 9894, vocab size: 2098\n",
      "Number of validation sentences: 1262\n",
      "Number of test sentences: 1230\n"
     ]
    }
   ],
   "source": [
    "# load text and emotion data for training, validation and test sets\n",
    "text_train, emotion_train = load_file(\"./dataset/train.txt\")\n",
    "text_validation , emotion_validation = load_file(\"./dataset/val.txt\")\n",
    "text_test, emotion_test = load_file(\"./dataset/test.txt\")\n",
    "\n",
    "# build vocabulary for each set\n",
    "lst_train_with_unk = list(yield_tokens_with_unknown(list(yield_tokens(text_train, threshold=size_threshold)), \n",
    "                                                    threshold=appearance_threshold))\n",
    "vocab_train = build_vocab_from_iterator(lst_train_with_unk, specials=[\"<pad>\", \"<unk>\"])\n",
    "\n",
    "lst_val = list(yield_tokens(text_validation, threshold=size_threshold)) # we do not add <unk> in the validation and test sets, we just replace unknown words (not in the train set) by <unk>\n",
    "lst_test = list(yield_tokens(text_test, threshold=size_threshold))\n",
    "\n",
    "# convert to OneHotEncoder dataset\n",
    "nb_train_sentences = sum(len(sentence.split(\" \"))<=size_threshold for sentence in text_train)\n",
    "nb_validation_sentences = sum(len(sentence.split(\" \"))<=size_threshold for sentence in text_validation)\n",
    "nb_test_sentences = sum(len(sentence.split(\" \"))<=size_threshold for sentence in text_test)\n",
    "\n",
    "print(f\"Number of training sentences: {nb_train_sentences}, vocab size: {len(vocab_train)}\")\n",
    "print(f\"Number of validation sentences: {nb_validation_sentences}\")\n",
    "print(f\"Number of test sentences: {nb_test_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27af9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only emotion from sentences of length <= size_threshold\n",
    "emotion_train = [emotion_train[i] for i in range(len(text_train)) if len(text_train[i].split(\" \"))<=size_threshold]\n",
    "emotion_validation = [emotion_validation[i] for i in range(len(text_validation)) if len(text_validation[i].split(\" \"))<=size_threshold]\n",
    "emotion_test = [emotion_test[i] for i in range(len(text_test)) if len(text_test[i].split(\" \"))<=size_threshold]\n",
    "\n",
    "lst_train_emotion = list(yield_tokens(emotion_train))\n",
    "emotion_classes = build_vocab_from_iterator(lst_train_emotion) # we build the emotion classes only with the training set\n",
    "\n",
    "lst_val_emotion = list(yield_tokens(emotion_validation))\n",
    "lst_test_emotion = list(yield_tokens(emotion_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655581e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionsDataset(tokenized_text=lst_train_with_unk, \n",
    "                              vocab=vocab_train, \n",
    "                              nb_sentences=nb_train_sentences, \n",
    "                              tokenized_emotion=lst_train_emotion, \n",
    "                              emotion_classes=emotion_classes)\n",
    "validation_dataset = EmotionsDataset(tokenized_text=lst_val, \n",
    "                                    vocab=vocab_train, \n",
    "                                    nb_sentences=nb_validation_sentences, \n",
    "                                    tokenized_emotion=lst_val_emotion, \n",
    "                                    emotion_classes=emotion_classes)\n",
    "test_dataset = EmotionsDataset(tokenized_text=lst_test, \n",
    "                              vocab=vocab_train, \n",
    "                              nb_sentences=nb_test_sentences, \n",
    "                              tokenized_emotion=lst_test_emotion, \n",
    "                              emotion_classes=emotion_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feedf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence of the training set (text):\n",
      "i didnt feel humiliated\n",
      "Associated emotion: sadness\n",
      "First sentence of the training set (one-hot encoded):\n",
      "(tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]]), tensor([1., 0., 0., 0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "print(\"First sentence of the training set (text):\")\n",
    "print(text_train[0])\n",
    "print(\"Associated emotion:\", emotion_train[0])\n",
    "print(\"First sentence of the training set (one-hot encoded):\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124ff93",
   "metadata": {},
   "source": [
    "Le réseau de neurone prend en entrée de façon récurrente un vecteur : $(batch_{size},vocab_{size})$. Soit par exemple pour un $batch_{size} = 10$, $sentence_{size} = 20$ :\n",
    " les dix premiers mots, les dix seconds mots, ..., les dix vingtièmes mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73601a92",
   "metadata": {},
   "source": [
    "2. Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f268ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters definition\n",
    "emb_size = 64 # arbitrary choice\n",
    "hidden_size = 128 # arbitrary choice\n",
    "rnn_layers = 1\n",
    "\n",
    "eta = 1e-3\n",
    "n_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "390dc8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xpu device\n",
      "ElmanRNN(\n",
      "  (i2e): Linear(in_features=2098, out_features=64, bias=True)\n",
      "  (i2h): Linear(in_features=192, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=192, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = ElmanRNN(input_size=len(vocab_train),hidden_size=hidden_size,emb_size=emb_size,output_size=len(emotion_classes),num_layers=rnn_layers, device=device).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c2a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optim = torch.optim.Adam(model.parameters(), lr=eta) # trouver la méthode pour faire un équilibrage des classes dans un second temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc3a8e",
   "metadata": {},
   "source": [
    "3. Test du modèle (sans/avec récurrence, phrase unique/batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e402ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7548, -1.7721, -1.7480, -1.8273, -1.8295, -1.8225], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test with no recurrence (single word to predict the emotion, batch_size=1)\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y, lengths = batch\n",
    "    pred, _ = model.forward_sequence(X.to(device), lengths=lengths.to(device), recurrence=False)\n",
    "    pred = torch.nn.functional.log_softmax(pred, dim=1)\n",
    "    for prediction, target in zip(pred, y):\n",
    "        print(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40476e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7711, -1.7449, -1.7417, -1.8440, -1.7919, -1.8634], device='xpu:0') tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test of the network, calling it with recurrence (batch_size=1)\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y, lengths = batch\n",
    "    pred, _ = model.forward_sequence(X.to(device), lengths=lengths.to(device), recurrence=True)\n",
    "    pred = torch.nn.functional.log_softmax(pred, dim=1)\n",
    "    for prediction, target in zip(pred, y):\n",
    "        print(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ee4c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7555, -1.7445, -1.7488, -1.8387, -1.7967, -1.8734], device='xpu:0') tensor([0., 1., 0., 0., 0., 0.])\n",
      "tensor([-1.7606, -1.7444, -1.7439, -1.8403, -1.7988, -1.8696], device='xpu:0') tensor([0., 0., 0., 1., 0., 0.])\n",
      "tensor([-1.7636, -1.7463, -1.7428, -1.8288, -1.7991, -1.8768], device='xpu:0') tensor([0., 1., 0., 0., 0., 0.])\n",
      "tensor([-1.7647, -1.7460, -1.7341, -1.8492, -1.8039, -1.8598], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n",
      "tensor([-1.7637, -1.7507, -1.7382, -1.8357, -1.7954, -1.8739], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n",
      "tensor([-1.7679, -1.7420, -1.7373, -1.8387, -1.7978, -1.8744], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n",
      "tensor([-1.7604, -1.7441, -1.7415, -1.8465, -1.7984, -1.8669], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n",
      "tensor([-1.7641, -1.7420, -1.7460, -1.8445, -1.7923, -1.8687], device='xpu:0') tensor([0., 0., 0., 1., 0., 0.])\n",
      "tensor([-1.7640, -1.7412, -1.7459, -1.8383, -1.8013, -1.8665], device='xpu:0') tensor([0., 1., 0., 0., 0., 0.])\n",
      "tensor([-1.7634, -1.7451, -1.7481, -1.8343, -1.8001, -1.8656], device='xpu:0') tensor([1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test of the network, calling it with recurrence (batch_size>1)\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab_train))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y, lengths = batch\n",
    "    pred, _ = model.forward_sequence(X.to(device), lengths=lengths.to(device), recurrence=True)\n",
    "    pred = torch.nn.functional.log_softmax(pred, dim=1)\n",
    "    for prediction, target in zip(pred, y):\n",
    "        print(prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8f957",
   "metadata": {},
   "source": [
    "#### Entrainer le réseau sur la classification des émotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7) Pipeline\n",
    "# -------------------------\n",
    "def run_pipeline(train_path: str, val_path: str, test_path: str,\n",
    "                 sep: str = \";\",\n",
    "                 specials: list[str] = [\"<pad>\", \"<unk>\"],\n",
    "                 min_freq: int = 1,\n",
    "                 batch_size: int = 64,\n",
    "                 hidden_size: int = 128,\n",
    "                 n_epochs: int = 20,\n",
    "                 lr: float = 1e-3,\n",
    "                 device = None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 1) load\n",
    "    train_texts, train_labels = load_file(train_path, sep=sep)\n",
    "    val_texts, val_labels = load_file(val_path, sep=sep)\n",
    "    test_texts, test_labels = load_file(test_path, sep=sep)\n",
    "\n",
    "    # Check if datasets are empty\n",
    "    if not train_texts or not train_labels:\n",
    "        print(f\"Error: Training data is empty after loading from {train_path}. Please check the file path and separator.\")\n",
    "        return None, None, None, None, None # Return None for all outputs\n",
    "    if not val_texts or not val_labels:\n",
    "        print(f\"Error: Validation data is empty after loading from {val_path}. Please check the file path and separator.\")\n",
    "        return None, None, None, None, None # Return None for all outputs\n",
    "    if not test_texts or not test_labels:\n",
    "        print(f\"Error: Test data is empty after loading from {test_path}. Please check the file path and separator.\")\n",
    "        return None, None, None, None, None # Return None for all outputs\n",
    "\n",
    "\n",
    "    # 2) vocab\n",
    "    # tokens iterator uses only train\n",
    "    voc = build_vocab_from_iterator(yield_tokens(train_texts), specials=specials)\n",
    "    print(\"Vocab size:\", len(voc))\n",
    "    # reverse mapping\n",
    "    itos = {idx: token for token, idx in voc.items()}\n",
    "\n",
    "    # classes vocab\n",
    "    classes = build_vocab_from_iterator(yield_tokens(train_labels), specials=[])\n",
    "    label2idx = {lab: idx for lab, idx in classes.items()}\n",
    "    idx2label = {v:k for k,v in label2idx.items()}\n",
    "    print(\"Num classes:\", len(label2idx), \"classes:\", list(label2idx.keys()))\n",
    "\n",
    "    # 3) datasets + loaders\n",
    "    train_ds = EmotionsDataset(train_texts, train_labels, voc, label2idx)\n",
    "    val_ds = EmotionsDataset(val_texts, val_labels, voc, label2idx)\n",
    "    test_ds = EmotionsDataset(test_texts, test_labels, voc, label2idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_pad)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n",
    "\n",
    "    # 4) model\n",
    "    vocab_size = len(voc)\n",
    "    num_classes = len(label2idx)\n",
    "    model = ElmanRNN(input_size=vocab_size, hidden_size=hidden_size, output_size=num_classes, device=device).to(device)\n",
    "\n",
    "    # 5) class weights\n",
    "    class_weights = compute_class_weights(train_labels, label2idx).to(device)\n",
    "    # We'll handle potential zero weight (class not present) by replacing zeros with 0.0\n",
    "    class_weights[class_weights == 0.0] = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 6) train loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience = 0\n",
    "    early_stop = 5\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_path = \"best_elman_rnn.pt\"\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
    "        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"Epoch {epoch:02d} | Train loss {tr_loss:.4f} acc {tr_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(\"  -> saved model\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop:\n",
    "                print(\"Early stop\")\n",
    "                break\n",
    "\n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "    test_loss, test_acc, y_true, y_pred = eval_epoch(model, test_loader, criterion, device)\n",
    "    print(f\"Test loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[idx2label[i] for i in range(len(idx2label))]))\n",
    "\n",
    "    # Visualisations demandées\n",
    "    plot_learning(history)\n",
    "    plot_confusion(y_true, y_pred, [idx2label[i] for i in range(len(idx2label))])\n",
    "\n",
    "    # Embedding matrix - ici l'embedding est implicite : la \"matrice d'embedding\"\n",
    "    # correspond à la matrice de poids de i2h / i2o si on voulait en extraire\n",
    "    # Ici on peut extraire les colonnes correspondantes en simulant une one-hot for each token:\n",
    "    # For token i, combined = [one_hot(i), zeros(hidden)] so i2h.weight[:, :vocab_size] acts like embedding\n",
    "    with torch.no_grad():\n",
    "        # i2h weight shape: (hidden_size, input_size + hidden_size)\n",
    "        w_i2h = model.i2h.weight[:, :vocab_size].cpu().numpy().T  # (vocab_size, hidden_size)\n",
    "        # optionally reduce dim to visualize\n",
    "        visualize_embeddings_pca_tsne(w_i2h, itos, top_n=200)\n",
    "\n",
    "    return model, voc, itos, label2idx, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a484d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = data.DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a9209b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.804481  [   10/ 9894]\n",
      "loss: 1.538822  [ 1010/ 9894]\n",
      "loss: 1.601116  [ 2010/ 9894]\n",
      "loss: 1.574642  [ 3010/ 9894]\n",
      "loss: 1.339141  [ 4010/ 9894]\n",
      "loss: 1.844930  [ 5010/ 9894]\n",
      "loss: 1.358010  [ 6010/ 9894]\n",
      "loss: 1.369007  [ 7010/ 9894]\n",
      "loss: 1.370254  [ 8010/ 9894]\n",
      "loss: 1.572130  [ 9010/ 9894]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.587962 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.400554  [   10/ 9894]\n",
      "loss: 1.558860  [ 1010/ 9894]\n",
      "loss: 1.406812  [ 2010/ 9894]\n",
      "loss: 1.690402  [ 3010/ 9894]\n",
      "loss: 1.380904  [ 4010/ 9894]\n",
      "loss: 1.558254  [ 5010/ 9894]\n",
      "loss: 1.616880  [ 6010/ 9894]\n",
      "loss: 1.482635  [ 7010/ 9894]\n",
      "loss: 1.263819  [ 8010/ 9894]\n",
      "loss: 1.685166  [ 9010/ 9894]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.585503 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.410831  [   10/ 9894]\n",
      "loss: 1.634604  [ 1010/ 9894]\n",
      "loss: 1.789904  [ 2010/ 9894]\n",
      "loss: 1.749041  [ 3010/ 9894]\n",
      "loss: 1.780067  [ 4010/ 9894]\n",
      "loss: 1.462513  [ 5010/ 9894]\n",
      "loss: 1.410371  [ 6010/ 9894]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurrence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     val_accuracy, val_loss \u001b[38;5;241m=\u001b[39m test(validation_loader, model, loss_func, device, recurrence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Documents\\INSA\\5A\\BioInspired-Machine-Learning\\Projet_NLP\\RNN.py:160\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, device, recurrence, verbose)\u001b[0m\n\u001b[0;32m    157\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    162\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\torch-xpu\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\torch-xpu\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\torch-xpu\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "counter = 0\n",
    "train_losses, val_losses = [], [] # to store the losses for plotting later\n",
    "# writer = torch.utils.tensorboard.SummaryWriter()\n",
    "for t in range(n_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_loader, model, loss_func, optim, device, recurrence=True)\n",
    "    val_accuracy, val_loss = test(validation_loader, model, loss_func, device, recurrence=True)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    stop, best_score, counter = early_stopping(best_score, val_accuracy, threshold=0.001, patience=5, counter=counter)\n",
    "    if stop:\n",
    "        break\n",
    "\n",
    "    # writer.add_scalar('Loss/train', train_loss, t)\n",
    "    # writer.add_scalar('Loss/validation', val_loss, t)\n",
    "    # writer.add_scalar('Accuracy/validation', val_accuracy, t)\n",
    "\n",
    "print(f\"Best validation accuracy reached: {best_score*100:>0.1f}%, after {t+1} epochs\")\n",
    "\n",
    "# visualize the training curves with tensorboard\n",
    "# writer.flush()\n",
    "# writer.close()\n",
    "# tensorboard --logdir=runs (launch this command in the terminal)\n",
    "\n",
    "# visualize the training and validation loss curves\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add computation of confusion matrix (to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters tuning (to be done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937e573",
   "metadata": {},
   "source": [
    "#### Optimiser, analyser les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiter l'effet du déséquilibre des classes (loss adaptée)\n",
    "# changer le threshold de filtrage des mots rares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53d978",
   "metadata": {},
   "source": [
    "#### Approfondissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da591ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrice d'embedding : observer la représentation des mots dans l'espace des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5630bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apprentissage auto-supervisé des embeddings (word2vec, GloVe, FastText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-xpu)",
   "language": "python",
   "name": "torch-xpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
